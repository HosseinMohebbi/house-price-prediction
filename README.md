# ğŸ  House Price Prediction â€“ King County

## ğŸ“Œ Project Overview

This project focuses on predicting house prices in King County, USA using multiple regression-based machine learning models.

The objective was to:

- Handle skewed target distribution
- Compare linear vs ensemble models
- Perform hyperparameter tuning
- Evaluate models on both log-transformed and real price scales
- Select the best-performing model based on generalization performance

---

## ğŸ“Š Dataset

**Dataset:** House Sales in King County, USA  
**Source:** Kaggle â€“ House Sales in King County, USA  
ğŸ”— https://www.kaggle.com/datasets/harlfoxem/housesalesprediction  

- 21 Features  
- 21,614 rows  
- Target variable: `price`

---

## ğŸ§¹ Data Preprocessing

- Checked for missing values  
- Log transformation applied to target variable:


```python
df["price"] = np.log1p(df["price"])

```
## ğŸ¤– Models Implemented

The following regression models were trained and evaluated:

- Linear Regression

- Ridge Regression

- Random Forest Regressor

- Gradient Boosting Regressor (Tuned)

## Model Performance Comparison

Gradient Boosting achieved the best overall performance across both log and real price scales.

### Log Scale Performance:

| Model             | Train RÂ² | Test RÂ²    |
| ----------------- | -------- | ---------- |
| Linear Regression | 0.8757   | 0.8743     |
| Ridge Regression  | 0.8757   | 0.8743     |
| Random Forest     | 0.9539   | 0.8841     |
| Gradient Boosting | 0.9256   | **0.9009** |

<br>

### Real Price Scale Performance:

| Model                 | RÂ²         | MAE ($)    | RMSE ($)    |
| --------------------- | ---------- | ---------- | ----------- |
| Linear Regression     | 0.6498     | 82,164     | 230,104     |
| Ridge Regression      | 0.6499     | 82,163     | 230,069     |
| Random Forest         | 0.8230     | 77,398     | 163,597     |
| **Gradient Boosting** | **0.8848** | **69,137** | **131,991** |


## ğŸ“ˆ Target Distribution Before & After Log Transformation

![boxplot-before-log](images/boxplot-before-log.png)

![boxplot-after-log](images/boxplot-after-log.png)

## ğŸ”„ Cross-Validation

To ensure model robustness and avoid overfitting, 5-Fold Cross Validation was applied:

- Ridge Regression: automatic alpha selection via RidgeCV (cv=5)
- Random Forest: RandomizedSearchCV with 5-fold CV
- Gradient Boosting: evaluated on hold-out test set after tuning

Cross-validation helped ensure stable generalization performance across folds.


## ğŸ† Final Model

The tuned Gradient Boosting Regressor achieved the best generalization performance:

- Highest RÂ² on test data

- Lowest MAE

- Lowest RMSE

- Balanced bias-variance tradeoff

It was selected as the final model.

## ğŸ“Š Key Insights

- Log transformation significantly improved model stability.

- Linear models performed well but lacked nonlinear modeling power.

- Random Forest showed slight overfitting.

- Gradient Boosting provided the best overall predictive performance.

## ğŸ›  Technologies Used

- Python

- Pandas

- NumPy

- Scikit-learn

- Matplotlib / Seaborn

- Jupyter Notebook

## ğŸš€ How to Run the Project

```bash
git clone <your-repo-link>
cd <your-project-folder>
pip install -r requirements.txt
jupyter notebook
```

## ğŸ“‚ Repository Structure


```text
â”œâ”€â”€ data
â”‚   â”œâ”€â”€ processed
â”‚   â”‚   â”œâ”€â”€ scaler.pkl
â”‚   â”‚   â”œâ”€â”€ x_train.pkl
â”‚   â”‚   â”œâ”€â”€ x_test.pkl
â”‚   â”‚   â”œâ”€â”€ x_train_scaled.pkl
â”‚   â”‚   â”œâ”€â”€ x_test_scaled.pkl
â”‚   â”‚   â”œâ”€â”€ y_train.pkl
â”‚   â”‚   â””â”€â”€ y_test.pkl
â”‚   â””â”€â”€ raw
â”‚       â””â”€â”€ kc_house_data.csv
â”‚
â”œâ”€â”€ images
â”‚   â”œâ”€â”€ boxplot-before-log.png
â”‚   â””â”€â”€ boxplot-after-log.png
â”‚
â”œâ”€â”€ notebooks
â”‚   â”œâ”€â”€ 01_eda.ipynb
â”‚   â””â”€â”€ 02_modeling.ipynb
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

## ğŸ‘¨â€ğŸ’» Author

Hossein Mohebbi

GitHub: https://github.com/HosseinMohebbi